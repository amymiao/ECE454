Q1)
It is important to ifdef out methods and data-structures that are not being used because there is a high possibility that the code for a previous version would be interacting with the current version. For example, if global locks was coded and then list locks were being implemented. The global lock code would be present affecting the thread function in randtrack thereby giving you an inaccurate picture. Also, if methods/procedures were previously modified – there is a high chance that the same procedure would need to be used in another implementation causing problems again. Also, if you declare data structures and do not use them – they take up memory. Additionally, ifdef helps isolate different implementations of the code.

Q2)
The difficulty of implementing transactional memory was about the same when compared to implementing the global lock. The critical section where the global lock and unlock code existed had to be wrapped around with __transaction_atomic{}  instead. This was easier to implement as we don’t have to worry about locking and unlocking before and after a critical section.

Q3)
It is not possible to correctly implement list level locks without knowing the internal implementation. Vital information such as how the hash index is calculated is not revealed to external users. List level locking is based on the hash index in order to work properly. 

Q4)
The insert function takes in a sample and the lookup function returns a pointer to a sample. This does not give any information about the hash index. Therefore, it is not possible to obtain the information to do it this way.

Q5)
The counter for each sample is incremented after the lookup and insert is done. Simply implementing this with a lookup and insert function would not be enough if the counter is incremented externally. The lookup and insert function would ensure that accessing or modifying the list would be thread safe. However, this does not ensure that the counter will be incremented in a thread safe manner. Therefore, if the counter is external then this method will not work.

Q6)
Adding new methods to the hash class allows us to view the internal implementation of the hash. This allows us to see how the hash index is calculated and gives us the ability to lock the list based on the hash index which is what we need to implement list level locks. This allows complete thread safety throughout the duration of looking up, inserting and incrementing the counter. We would lock the corresponding list. We can find out which list to lock based on the sample information (use this to calculate the hash index and lock that corresponding list). We can unlock the list after the sample has completed incrementing. This will ensure that only one thread accesses a list at a time.

Q7)
Implementing TM locks was significantly easier as no functions had to be modified and the inner workings of the hash class did not need to be understood. 
 
Q8)
The nice part about the reduction approach is no synchronization is needed during collection of samples because each thread is working on its own private copy of the hash table. This allows work to be done in parallel without enforcing atomicity which slows down the work. The bad part about this approach is that the combining time at the end (summing entries if they occur in two tables/inserting them into the final table if it doesn’t exist) may start to take a significant toll for large computations thereby negating the time savings gained from the purely parallel part of the computation.

Q9) All runtimes are in seconds:
Baseline:
Runtime: 17.8
Overhead: 1

Global Lock:
Runtime: 19.47
Overhead: 1.093820225

Transactional Memory
Runtime: 21.21
Overhead: 1.191573034

List Lock
Runtime: 20
Overhead: 1.123595506

Element Lock
Runtime: 21.62
Overhead: 1.214606742

Reduction
Runtime: 17.81
Overhead: 1.000561798

Q10) Generally, runtime decreased meaning performance increased with number of threads.  Exceptions to this were:
- Global locks. The performance of 4 threads was worse than both 1 and 2.
- What is happening is that there are multiple threads waiting to execute, however access to the list has to be atomic, so most of the threads are just waiting to acquire the lock. The overhead of threads trying to acquire the lock increases the total time.  * (look into more)

Q11) By doubling the samples to skip, the percentage overhead for each parallel version with one thread was cut in half, except for the reduction which slightly decreased but was already at almost 0%.

From the code, we can see that changing samples_to_skip only affects the computation of "rnum".  As a result, the locks and thread creation code for the 1 thread case will always add a constant latency overhead.  Additionally, we see that when we double "samples_to_skip", the total execution time doubles as well.  This means, the new % overhead of parallelization changes from:

samples_to_skip = 50: const/(base_execution_time)
samples_to_skip = 100: const/(2*base_execution_time).

This simplifies to a halving of the overhead.  We can further prove this constant latency by examining changes to samples_to_skip for one of the parallelization methods (in this case, global locks):

samples_to_skip | time (baseline) | time (parallelization) | difference
50              | 17.74           | 19.42                  | 1.7
100             | 35.05           | 36.71                  | 1.7
200             | 69.67           | 71.16                  | 1.5
400             | 138.65          | 140.16                 | 1.5

Q12) Based on our results, we found that reduction was always better.  The reduction method allows the threads to work on separate sections of data, allowing them to operate at maximum speed since they do not depend on each other.  
